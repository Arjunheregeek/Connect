"""
Enhanced Executor Node for LangGraph - Multi-Tool Execution with Priority Handling

This node reads sub_queries from state (generated by Enhanced Planner) and executes
them via MCP tools with intelligent priority handling and result aggregation.

Features:
- Priority-based execution (Priority 1 first, then 2, then 3)
- Parallel execution within same priority level
- Result aggregation based on execution_strategy (intersect/union)
- Proper MCP client integration

Workflow Integration:
Enhanced Planner → [THIS NODE] → Tool Results → Synthesizer
"""

from typing import Dict, Any, List, Set
import sys
from pathlib import Path
import asyncio

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import AgentState
try:
    from agent.state import AgentState
except ImportError:
    from typing import TypedDict
    class AgentState(TypedDict, total=False):
        user_query: str
        workflow_status: str
        sub_queries: List[Dict[str, Any]]
        execution_strategy: str
        tool_results: List[Dict[str, Any]]
        accumulated_data: List[Any]
        errors: List[str]
        filters: Dict[str, Any]
        planning_metadata: Dict[str, Any]

# Import MCP Client
try:
    from agent.mcp_client import MCPClient
    MCP_AVAILABLE = True
except ImportError:
    print("⚠️  Warning: MCP Client not available")
    MCP_AVAILABLE = False
    MCPClient = None


async def enhanced_executor_node(state: AgentState) -> AgentState:
    """
    Enhanced Executor node that executes sub-queries with priority handling.
    
    Execution Flow:
    1. Read sub_queries from state (from enhanced_planner_node)
    2. Group by priority (1, 2, 3)
    3. Execute each priority level in parallel
    4. Aggregate results based on execution_strategy:
       - parallel_intersect: Find intersection of all results (AND logic)
       - parallel_union: Find union of all results (OR logic)
       - sequential: Execute in order, pass results between steps
    5. Store results in state for synthesizer
    
    State Input (from planner):
        - sub_queries: List[Dict] (tool, params, priority, etc.)
        - execution_strategy: str (parallel_intersect/parallel_union)
    
    State Output (to synthesizer):
        - tool_results: List[Dict] (all tool execution results)
        - accumulated_data: List[Any] (aggregated person IDs or data)
        - workflow_status: str (updated to 'tools_complete')
    
    Args:
        state: Current AgentState with sub_queries
        
    Returns:
        Updated AgentState with tool execution results
    """
    
    # Update status
    state['workflow_status'] = 'executing_tools'
    
    # Check if MCP client is available
    if not MCP_AVAILABLE or not MCPClient:
        if 'errors' not in state:
            state['errors'] = []
        state['errors'].append("MCP Client not available")
        state['workflow_status'] = 'error'
        return state
    
    try:
        sub_queries = state.get('sub_queries', [])
        execution_strategy = state.get('execution_strategy', 'parallel_union')
        
        if not sub_queries:
            if 'errors' not in state:
                state['errors'] = []
            state['errors'].append("No sub-queries to execute")
            state['workflow_status'] = 'error'
            return state
        
        print(f"\n✓ Executing {len(sub_queries)} sub-queries with strategy: {execution_strategy}")
        
        # Group sub-queries by priority
        priority_groups = {}
        for sq in sub_queries:
            priority = sq.get('priority', 1)
            if priority not in priority_groups:
                priority_groups[priority] = []
            priority_groups[priority].append(sq)
        
        # Execute each priority group
        all_results = []
        all_person_ids = {}  # Dict to track person IDs from each sub-query
        
        # Connect to LOCAL MCP server (not production)
        async with MCPClient(base_url="http://localhost:8000") as mcp_client:
            for priority in sorted(priority_groups.keys()):
                sub_queries_batch = priority_groups[priority]
                print(f"\n  ⚡ Executing Priority {priority}: {len(sub_queries_batch)} sub-queries")
                
                # Execute all sub-queries in this priority level in parallel
                tasks = []
                for sq in sub_queries_batch:
                    task = execute_single_subquery(mcp_client, sq)
                    tasks.append(task)
                
                # Wait for all tasks in this priority to complete
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Process results
                for sq, result in zip(sub_queries_batch, batch_results):
                    if isinstance(result, Exception):
                        print(f"    ❌ {sq.get('tool')}: {str(result)}")
                        all_results.append({
                            'sub_query': sq.get('sub_query', ''),
                            'tool_name': sq.get('tool', 'unknown'),
                            'params': sq.get('params', {}),
                            'success': False,
                            'error': str(result),
                            'person_ids': [],
                            'priority': priority
                        })
                    else:
                        print(f"    ✓ {sq.get('tool')}: {result.get('person_count', 0)} results")
                        all_results.append(result)
                        
                        # Track person IDs for aggregation
                        sq_key = f"{sq.get('tool')}_{priority}"
                        all_person_ids[sq_key] = set(result.get('person_ids', []))
        
        # Aggregate results based on execution strategy
        final_person_ids = aggregate_results(all_person_ids, execution_strategy, priority_groups)
        
        # Store results in state
        if 'tool_results' not in state:
            state['tool_results'] = []
        state['tool_results'] = all_results
        
        if 'accumulated_data' not in state:
            state['accumulated_data'] = []
        state['accumulated_data'] = list(final_person_ids)
        
        # Update workflow status
        state['workflow_status'] = 'tools_complete'
        
        # Log execution summary
        successful = len([r for r in all_results if r.get('success', False)])
        print(f"\n✓ Execution Complete:")
        print(f"  - Total sub-queries: {len(sub_queries)}")
        print(f"  - Successful: {successful}")
        print(f"  - Failed: {len(all_results) - successful}")
        print(f"  - Final person count: {len(final_person_ids)}")
        
    except Exception as e:
        # Handle unexpected errors
        if 'errors' not in state:
            state['errors'] = []
        state['errors'].append(f"Execution error: {str(e)}")
        state['workflow_status'] = 'error'
        
        # Add debug info
        import traceback
        print(f"❌ Execution Error: {str(e)}")
        print(traceback.format_exc())
    
    return state


async def execute_single_subquery(mcp_client: Any, sub_query: Dict[str, Any]) -> Dict[str, Any]:
    """
    Execute a single sub-query via MCP client.
    
    Args:
        mcp_client: Initialized MCP client
        sub_query: Sub-query dict with tool, params, priority
        
    Returns:
        Result dict with success status and person IDs
    """
    tool_name = sub_query.get('tool', '')
    params = sub_query.get('params', {})
    priority = sub_query.get('priority', 1)
    
    print(f"      🚀 Starting execution: {tool_name} with params: {params}")
    
    try:
        # Map tool name to MCP client method
        tool_method = getattr(mcp_client.tools, tool_name, None)
        
        if not tool_method:
            print(f"      ❌ Tool method not found: {tool_name}")
            return {
                'sub_query': sub_query.get('sub_query', ''),
                'tool_name': tool_name,
                'params': params,
                'success': False,
                'error': f"Tool '{tool_name}' not found",
                'person_ids': [],
                'priority': priority
            }
        
        # Execute the tool
        response = await tool_method(**params)
        
        # DEBUG: Always print response information (before success check)
        print(f"\n      🔍 DEBUG - Tool: {tool_name}")
        print(f"      🔍 Response.success: {response.success}")
        print(f"      🔍 Response.data: {response.data}")
        print(f"      🔍 Response.error_code: {response.error_code}")
        print(f"      🔍 Response.error_message: {response.error_message}")
        
        if response.data:
            if isinstance(response.data, dict):
                print(f"      🔍 Data keys: {list(response.data.keys())}")
                if 'content' in response.data:
                    print(f"      🔍 Content: {response.data['content']}")
        
        # Check if successful
        if response.success and response.data:
            # Extract person IDs from response
            person_ids = extract_person_ids_from_response(response.data)
            
            return {
                'sub_query': sub_query.get('sub_query', ''),
                'tool_name': tool_name,
                'params': params,
                'success': True,
                'response_data': response.data,
                'person_ids': person_ids,
                'person_count': len(person_ids),
                'priority': priority
            }
        else:
            return {
                'sub_query': sub_query.get('sub_query', ''),
                'tool_name': tool_name,
                'params': params,
                'success': False,
                'error': response.error or "No data returned",
                'person_ids': [],
                'priority': priority
            }
            
    except Exception as e:
        return {
            'sub_query': sub_query.get('sub_query', ''),
            'tool_name': tool_name,
            'params': params,
            'success': False,
            'error': str(e),
            'person_ids': [],
            'priority': priority
        }


def extract_person_ids_from_response(response_data: Any) -> List[int]:
    """
    Extract person IDs from MCP response data.
    
    MCP Response Format (from mcp_handlers.py):
        {
            "content": [{
                "type": "text",
                "text": "[{'person_id': 123, 'name': 'John', ...}, ...]"  # Stringified list
            }]
        }
    
    Args:
        response_data: Response data from MCP tool (dict with 'content' key)
        
    Returns:
        List of person IDs
    """
    person_ids = []
    
    try:
        # Step 1: Extract the stringified data from MCP response format
        if not isinstance(response_data, dict):
            print(f"  ⚠️  Expected dict, got {type(response_data)}")
            return person_ids
        
        # MCP response has 'content' array
        if 'content' not in response_data:
            print(f"  ⚠️  No 'content' field in response_data. Keys: {response_data.keys()}")
            return person_ids
        
        content = response_data['content']
        if not isinstance(content, list) or len(content) == 0:
            print(f"  ⚠️  'content' is not a list or is empty")
            return person_ids
        
        # Extract the 'text' field which contains the stringified data
        text_content = content[0].get('text', '')
        if not text_content:
            print(f"  ⚠️  No 'text' in content[0]")
            return person_ids
        
        # Step 2: Parse the stringified data
        # The text is a string representation of a Python list/dict
        import json
        import ast
        
        parsed_data = None
        
        print(f"  🔍 DEBUG - text_content type: {type(text_content)}")
        print(f"  🔍 DEBUG - text_content length: {len(text_content)}")
        print(f"  🔍 DEBUG - text_content preview: {text_content[:300]}...")
        
        # Try JSON first (in case server uses json.dumps)
        try:
            parsed_data = json.loads(text_content)
            print(f"  ✅ Successfully parsed with json.loads()")
        except (json.JSONDecodeError, ValueError) as json_err:
            print(f"  ⚠️  json.loads() failed: {json_err}")
            # Try ast.literal_eval for Python string representation
            try:
                parsed_data = ast.literal_eval(text_content)
                print(f"  ✅ Successfully parsed with ast.literal_eval()")
            except (ValueError, SyntaxError) as e:
                print(f"  ⚠️  ast.literal_eval() ALSO failed: {e}")
                print(f"  ⚠️  Text preview: {text_content[:200]}...")
                return person_ids
        
        # Step 3: Extract person IDs from parsed data
        print(f"  🔍 DEBUG - parsed_data type: {type(parsed_data)}")
        if isinstance(parsed_data, list):
            print(f"  🔍 DEBUG - parsed_data is a list with {len(parsed_data)} items")
            # Expected format: List[Dict] where each dict has 'person_id' or 'id'
            for item in parsed_data:
                if isinstance(item, dict):
                    pid = item.get('person_id') or item.get('id')
                    if pid is not None:
                        try:
                            person_ids.append(int(pid))
                        except (ValueError, TypeError):
                            print(f"  ⚠️  Invalid person_id: {pid}")
                elif isinstance(item, (int, str)):
                    # Sometimes might just be a list of IDs
                    try:
                        person_ids.append(int(item))
                    except (ValueError, TypeError):
                        pass
            print(f"  ✅ Extracted {len(person_ids)} person IDs from list")
        
        elif isinstance(parsed_data, dict):
            # Single person object or wrapper dict
            if 'people' in parsed_data:
                # Wrapped in 'people' key
                people_list = parsed_data['people']
                if isinstance(people_list, list):
                    for person in people_list:
                        if isinstance(person, dict):
                            pid = person.get('person_id') or person.get('id')
                            if pid is not None:
                                try:
                                    person_ids.append(int(pid))
                                except (ValueError, TypeError):
                                    pass
            elif 'person_id' in parsed_data or 'id' in parsed_data:
                # Single person object
                pid = parsed_data.get('person_id') or parsed_data.get('id')
                if pid is not None:
                    try:
                        person_ids.append(int(pid))
                    except (ValueError, TypeError):
                        pass
            else:
                print(f"  ⚠️  Dict format not recognized. Keys: {parsed_data.keys()}")
        
        else:
            print(f"  ⚠️  Unexpected parsed_data type: {type(parsed_data)}")
    
    except Exception as e:
        print(f"  ⚠️  Error extracting person IDs: {e}")
        import traceback
        print(traceback.format_exc())
    
    return person_ids


def aggregate_results(
    all_person_ids: Dict[str, Set[int]], 
    strategy: str,
    priority_groups: Dict[int, List[Dict]]
) -> Set[int]:
    """
    Aggregate person IDs based on execution strategy.
    
    Args:
        all_person_ids: Dict mapping sub-query keys to sets of person IDs
        strategy: Execution strategy (parallel_intersect/parallel_union/sequential)
        priority_groups: Sub-queries grouped by priority
        
    Returns:
        Final set of person IDs
    """
    if not all_person_ids:
        return set()
    
    if strategy == 'parallel_intersect':
        # Intersection: Only people who match ALL priority 1 queries
        priority_1_ids = [ids for key, ids in all_person_ids.items() if key.endswith('_1')]
        if priority_1_ids:
            result = set.intersection(*priority_1_ids) if len(priority_1_ids) > 1 else priority_1_ids[0]
        else:
            result = set()
        return result
    
    elif strategy == 'parallel_union':
        # Union: People who match ANY query
        all_ids = [ids for ids in all_person_ids.values()]
        if all_ids:
            result = set.union(*all_ids) if len(all_ids) > 1 else all_ids[0]
        else:
            result = set()
        return result
    
    elif strategy == 'sequential':
        # Sequential: Start with priority 1, filter with priority 2, etc.
        result = set()
        for priority in sorted(priority_groups.keys()):
            priority_ids = [ids for key, ids in all_person_ids.items() if key.endswith(f'_{priority}')]
            if priority_ids:
                priority_union = set.union(*priority_ids) if len(priority_ids) > 1 else priority_ids[0]
                if priority == 1:
                    result = priority_union
                else:
                    result = result.intersection(priority_union)
        return result
    
    else:
        # Default to union
        all_ids = [ids for ids in all_person_ids.values()]
        return set.union(*all_ids) if all_ids else set()


# Standalone test function
async def test_enhanced_executor():
    """Test the enhanced executor node with mock data."""
    
    print("="*70)
    print("ENHANCED EXECUTOR NODE - STANDALONE TEST")
    print("="*70)
    print("\nNote: This requires MCP server to be running on port 8000")
    print("="*70)
    
    # Create test state with sub-queries
    test_state: AgentState = {
        'user_query': 'Find Machine Learning experts',
        'workflow_status': 'planning_complete',
        'sub_queries': [
            {
                'sub_query': 'Find people with Machine Learning skills',
                'tool': 'find_people_by_skill',
                'params': {'skill': 'Machine Learning'},
                'priority': 1,
                'rationale': 'Direct skill match'
            },
            {
                'sub_query': 'Find people with leadership indicators',
                'tool': 'find_leadership_indicators',
                'params': {},
                'priority': 2,
                'rationale': 'Optional leadership filter'
            }
        ],
        'execution_strategy': 'parallel_union',  # Changed to union so we get all results
        'tool_results': [],
        'accumulated_data': [],
        'errors': []
    }
    
    # Run the executor node
    result_state = await enhanced_executor_node(test_state)
    
    # Display results
    print(f"\nStatus: {result_state['workflow_status']}")
    
    if result_state.get('errors'):
        print(f"Errors: {result_state['errors']}")
    else:
        print(f"\n✓ Tool Results: {len(result_state.get('tool_results', []))}")
        for i, result in enumerate(result_state.get('tool_results', []), 1):
            success = "✓" if result.get('success') else "❌"
            print(f"  {i}. {success} {result.get('tool_name')}: {result.get('person_count', 0)} results")
        
        print(f"\n✓ Final Aggregated Results: {len(result_state.get('accumulated_data', []))} people")
    
    print("="*70)


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_enhanced_executor())
